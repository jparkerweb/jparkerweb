<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Justin Parker's Portfolio</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="#about">About</a></li>
                <li><a href="#skills">Skills</a></li>
                <li><a href="#projects">Projects</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section id="about">
            <h1>Justin Parker</h1>
            <p>ğŸ¤– AI Enthusiast ğŸ¨ Front End Dev / Manager ğŸ² Avid Board Gamer ğŸ˜‡ Husband / Father / Christian</p>
        </section>
        <section id="skills">
            <h2>Skills</h2>
            <ul>
                <li>Front End Development</li>
                <li>AI and Machine Learning</li>
                <li>Project Management</li>
                <li>Board Game Design</li>
            </ul>
        </section>
        <section id="projects">
            <h2>ğŸ™ Checkout my GitHub Projects</h2>
            <ul>
                <li><a href="https://github.com/jparkerweb" target="_blank">https://github.com/jparkerweb</a></li>
            </ul>

            <h2>ğŸ§‘â€ğŸ’» Few Published NPM Packages</h2>
            <ul>
                <li>
                    <a href="https://www.npmjs.com/package/semantic-chunking" target="_blank">ğŸ± Semantic Chunking</a><br>
                    Semantically create chunks from large texts, which is useful for workflows involving large language models (LLMs).
                </li>
                <li>
                    <a href="https://www.npmjs.com/package/llm-distillery" target="_blank">ğŸ¶ LLM Distillery</a><br>
                    Use LLMs to distill large texts down to a manageable size by utilizing a map-reduce approach. This ensures that the text fits within a specified token limit, which is crucial when interfacing with Large Language Models (LLMs) in downstreams tasks.
                </li>
                <li>
                    <a href="https://www.npmjs.com/package/bedrock-wrapper" target="_blank">ğŸª¨ Bedrock Wrapper</a><br>
                    Bedrock Wrapper is an npm package that simplifies the integration of existing OpenAI-compatible API objects with AWS Bedrock's serverless inference LLMs. Follow the steps below to integrate into your own application, or alternativly use the <a href="https://github.com/jparkerweb/bedrock-proxy-endpoint" target="_blank">ğŸ”€ Bedrock Proxy Endpoint</a> project to spin up your own custom OpenAI server endpoint for even easier inference (using the standard baseUrl, and apiKey params).
                </li>
            </ul>
        </section>
        <section id="contact">
            <h2>Contact</h2>
            <p>Email: jparkerweb+github@gmail.com</p>
        </section>
    </main>
    <footer>
        <p>&copy; 2024 Justin Parker  â‹…  â‹…  All rights reserved  â‹…  â‹…  v0.0.2</p>
    </footer>
</body>
</html>
